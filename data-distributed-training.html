
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Data-Distributed Training &#8212; PyTorch Training Performance Guide</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Model-Distributed Training" href="model-distributed-training.html" />
    <link rel="prev" title="Mixed Precision" href="mixed-precision.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">PyTorch Training Performance Guide</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Home
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="quickstart.html">
   Quickstart
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lr-sched-and-optim.html">
   LR Schedulers, Adaptive Optimizers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mixed-precision.html">
   Mixed Precision
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Data-Distributed Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model-distributed-training.html">
   Model-Distributed Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradient-checkpoints.html">
   Gradient Checkpoints
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="jit.html">
   JIT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pruning.html">
   Model Pruning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="assorted-tricks.html">
   Assorted Tricks
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/data-distributed-training.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-parallelization-versus-model-parallelization">
   Data parallelization versus model parallelization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-it-works">
   How it works
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-distributed-part-1-process-initialization">
   Data distributed, part 1: process initialization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-distributed-part-2-process-synchronization">
   Data distributed, part 2: process synchronization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-about-dataparallel">
   What about DataParallel?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#benchmarks">
   Benchmarks
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Data-Distributed Training</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-parallelization-versus-model-parallelization">
   Data parallelization versus model parallelization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-it-works">
   How it works
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-distributed-part-1-process-initialization">
   Data distributed, part 1: process initialization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-distributed-part-2-process-synchronization">
   Data distributed, part 2: process synchronization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-about-dataparallel">
   What about DataParallel?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#benchmarks">
   Benchmarks
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="data-distributed-training">
<h1>Data-Distributed Training<a class="headerlink" href="#data-distributed-training" title="Permalink to this headline">¶</a></h1>
<p><strong>Distributed training</strong> is the set of techniques for training a deep learning model using multiple GPUs and/or multiple machines. Distributing training jobs allow you to push past the single-GPU memory and compute bottlenecks, expediting the training of larger models (or even making it possible to train them in the first place) by training across many GPUs simultaneously.</p>
<p>There are two types of distributed training that see use in production today. This chapter covers the better-known of the two techniques: <strong>data-distributed training</strong>. Data-distributed training works by initializing the same model on multiple different machines, slicing the batch up and backprogating on each machine simultaneously, collecting and averaging the resulting gradients, and then updating each local machine’s local copy of the model prior to the next round of training.</p>
<p>In native PyTorch this pattern is implemented by the <code class="docutils literal notranslate"><span class="pre">torch.nn.parallel.DistributedDataParallel</span></code> API.</p>
<p><a class="reference external" href="https://github.com/spellrun/deeplab-voc-2012">You can follow along in code by checking out the companion GitHub repo</a>.</p>
<p><strong>TLDR</strong>: data-distributed training is the best way to train models too large to fit on disk on a single machine. However, the network synchronization required have a very real efficiency cost, so you should only turn to using this technique once you have exhausted your ability to scale your training instance vertically (e.g. you are already working with the largest GPU instance available to you).</p>
<div class="section" id="data-parallelization-versus-model-parallelization">
<h2>Data parallelization versus model parallelization<a class="headerlink" href="#data-parallelization-versus-model-parallelization" title="Permalink to this headline">¶</a></h2>
<p>A model training job that uses data parallelization is executed on multiple GPUs simultaneously. Each GPU in the job receives its own independent slice of the data batch (a <strong>batch slice</strong>), which it uses to independently calculate a gradient update. For example, if you were to use two GPUs and a batch size of 32, one GPU would handle forward and back propagation on the first 16 records, and the second the last 16. These gradient updates are then synchronized among the GPUs, averaged together, and finally applied to the model.</p>
<p>The synchronization step is technically optional, but theoretically faster asynchronous update strategies are still an active area of research.</p>
<p>Data parallelization competes with <strong>model parallelization</strong> for mindshare. In model parallelization, the model training job is split on the model. Each GPU in the job receives a slice of the model - a subset of its layers. So for example, one GPU might be responsible for its output head, another might handle the input layers, and still another the hidden layers in between.</p>
<p>Data parallelization (aka data-distributed training) is the easier of these two techniques to implement. It requires no knowledge of the underlying network architecture to implement and has robust API implementations.</p>
<p>This chapter will cover data-distributed training only. A future chapter covers model-distributed training. Note that it’s also possible to use these two techniques simultaneously.</p>
</div>
<div class="section" id="how-it-works">
<h2>How it works<a class="headerlink" href="#how-it-works" title="Permalink to this headline">¶</a></h2>
<p>As far as I can tell, the first data parallelization technique to see adoption in deep learning was the <strong>parameter server strategy</strong> in TensorFlow. This technique predates TensorFlow itself: it was actually first implemented in its Google-internal predecessor, DistBelief, in 2012. This strategy is illustrated in the following diagram (taken from <a class="reference external" href="https://eng.uber.com/horovod/">here</a>):</p>
<p><img alt="Parameter server strategy" src="_images/parameter-server-strategy.avif" /></p>
<p>In the parameter server strategy there is a variable number of worker and parameter processes, with each worker process maintaining its own independent copy of the model in GPU memory. Gradient updates are computed as follows:</p>
<ol class="simple">
<li><p>Upon receiving the go signal, each worker process accumulates the gradients for its particular batch slice.</p></li>
<li><p>The workers sends their update to the parameter servers in a fan-out manner.</p></li>
<li><p>The parameter servers wait until they have all worker updates, then average the total gradient for the portion of the gradient update parameter space they are responsible for.</p></li>
<li><p>The gradient updates are fanned out to the workers, which sum them up and apply them to their in-memory copy of the model weights (thus keeping the worker models in sync).</p></li>
<li><p>Once every worker has applied the updates, a new batch of training is ready to begin.</p></li>
</ol>
<p>Whilst simple to implement, this strategy has some major limitations. Each additional parameter server requires <code class="docutils literal notranslate"><span class="pre">n_workers</span></code> additional network calls at each synchronization step — an <code class="docutils literal notranslate"><span class="pre">O(n^2)</span></code> complexity cost. Furthermore, computational sped was blocked on the slowest connection in the network. Large parameter server model training jobs proved to be very inefficient in practice, pushing net GPU utilization to 50% or below.</p>
<p>For the curious, the <a class="reference external" href="https://www.youtube.com/watch?v=jKV53r9-H14">Inside TensorFlow: tf.distribute.Strategy</a> tech talk has more details.</p>
<p>The 2017 Baidu paper <a class="reference external" href="https://web.archive.org/web/20180421011035/http://research.baidu.com/bringing-hpc-techniques-deep-learning/">Bringing HPC Techniques to Deep Learning</a> improved upon this strategy. This paper experimentally validated a new distributed training strategy that did away with the parameter server component.</p>
<p>In this strategy, every process is a worker process. Each process still maintains a complete in-memory copy of the model weights, but batch slice gradients updates are now synchronized and averaged directly on the worker processes themselves. This is achieved using a technique borrowed from the high-performance computing world: an <strong>all-reduce algorithm</strong>:</p>
<p><img alt="All-reduce diagram" src="_images/all-reduce.avif" /></p>
<p>This diagram shows one particular implementation of an all-reduce algorithm, ring all-reduce, in action. The algorithm provides an elegant way of synchronizing the state of a set of tensors among a collection of processes. The tensors are passed in a ring (hence the name) by a sequence of direct worker-to-woker connections. This eliminates the network bottleneck created by the worker-to-parameter-server connections, substantially improving performance.</p>
<p>In this scheme, gradient updates are computed as follows:</p>
<ol class="simple">
<li><p>Each worker maintains its own copy of the model weights and its own copy of the dataset.</p></li>
<li><p>Upon receiving the go signal, each worker process draws a disjoint batch from the dataset and computes a gradient for that batch.</p></li>
<li><p>The workers use an all-reduce algorithm to synchronize their individual gradients, computing the same average gradient on all nodes locally.</p></li>
<li><p>Each worker applies the gradient update to its local copy of the model.</p></li>
<li><p>The next batch of training is ready to begin.</p></li>
</ol>
<p>The <code class="docutils literal notranslate"><span class="pre">torch.nn.parallel.DistributedDataParallel</span></code> PyTorch API is an implementation of this strategy.</p>
<p>The great thing about this approach is that all-reduce is a well-understood HPC techniques with longstanding open source implementations. All-reduce is part of the <a class="reference external" href="https://en.wikipedia.org/wiki/Message_Passing_Interface">Message Passing Interface</a> (MPI) de facto standard, which is why PyTorch <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> offfers not one but three different backends: <a class="reference external" href="https://www.open-mpi.org/">Open MPI</a>, <a class="reference external" href="https://developer.nvidia.com/nccl">NVIDIA NCCL</a>, and <a class="reference external" href="https://github.com/facebookincubator/gloo">Facebook Gloo</a>.</p>
</div>
<div class="section" id="data-distributed-part-1-process-initialization">
<h2>Data distributed, part 1: process initialization<a class="headerlink" href="#data-distributed-part-1-process-initialization" title="Permalink to this headline">¶</a></h2>
<p>To demonstrate how the API works, we will build our way towards a complete distributed training script (which we will benchmark later). <a class="reference external" href="https://github.com/spellrun/deeplab-voc-2012">Here is the code on GitHub</a>.</p>
<p>The first and most complicated new thing you need to handle is process initialization. A vanilla PyTorch training script executes a single copy of its code inside of a single process. With parallelized training, the situation is more complicated: there are now as many simultaneous copies of the training script as there are GPUs in the training cluster, each one running in a different process.</p>
<p>Consider the following minimal example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># multi_init.py</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>

<span class="k">def</span> <span class="nf">init_process</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;gloo&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Initialize the distributed environment. &quot;&quot;&quot;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_ADDR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_PORT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;29500&#39;</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="n">init_process</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Rank </span><span class="si">{</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2"> process initialized.</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="c1"># rest of the training script goes here!</span>

<span class="n">WORLD_SIZE</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span>
        <span class="n">train</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">NUM_EPOCHS</span><span class="p">,</span> <span class="n">WORLD_SIZE</span><span class="p">),</span>
        <span class="n">nprocs</span><span class="o">=</span><span class="n">WORLD_SIZE</span><span class="p">,</span> <span class="n">join</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>In the world of MPI, <strong>world size</strong> is the number of processes being orchestrated, and <strong>(global) rank</strong> is the position of the current process in that world. So for example, if this script were to be executing on a beefy machine with four GPUs onboard, <code class="docutils literal notranslate"><span class="pre">WORLD_SIZE</span></code> would be <code class="docutils literal notranslate"><span class="pre">4</span></code> (because <code class="docutils literal notranslate"><span class="pre">torch.cuda.device_count()</span> <span class="pre">==</span> <span class="pre">4</span></code>), so <code class="docutils literal notranslate"><span class="pre">mp.spawn</span></code> would spawn <code class="docutils literal notranslate"><span class="pre">4</span></code> different processes, whose rank would be <code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">2</span></code>, or <code class="docutils literal notranslate"><span class="pre">3</span></code> respectively. The process with rank <code class="docutils literal notranslate"><span class="pre">0</span></code> is given a few extra responsibilities, and is therefore referred to as the <strong>master process</strong>.</p>
<p>The current process’s rank is passed through to the <code class="docutils literal notranslate"><span class="pre">spawn</span></code> entrypoint (in this case, the <code class="docutils literal notranslate"><span class="pre">train</span></code> method) as its first argument. Before <code class="docutils literal notranslate"><span class="pre">train</span></code> can actually do any work, it needs to first set up its connections to its peer processes. This is the responsibility of the <code class="docutils literal notranslate"><span class="pre">dist.init_process_group</span></code>. When run in the master process, this method sets up a socket listener on <code class="docutils literal notranslate"><span class="pre">MASTER_ADDR:MASTER_PORT</span></code> and starts handling connections from the other processes. Once all of the processes have connected, this method handles setting up the peer connections allowing the processes to communicate.</p>
<p>Note that this recipe only works for training on a single multi-GPU machine! The same machine is used to launch every single process in the job, so training can only leverage the GPUs connected to that specific machine. This makes things easy: setting up IPC is as easy as finding a free port on <code class="docutils literal notranslate"><span class="pre">localhost</span></code>, which is immediately visible to all processes on that machine. IPC across machines is more complicated, as it requires configuring an external IP address visible to all machines.</p>
<p>In this introductory tutorial we will focus specifically on the single-machine case, aka vertical scaling. Even on its own, vertical scaling is an extremely powerful tool. On the cloud, vertical scaling allows you to scale your deep learning training job all the way up to an A100x8. That’s a lot of deep learning horsepower!</p>
<p>We will discuss horizontal scaling with data parallelization in a future blog post. In the meantime, to see a code recipe showing it in action, check out <a class="reference external" href="https://web.archive.org/web/20200201184058/https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html">the PyTorch AWS tutorial</a>.</p>
</div>
<div class="section" id="data-distributed-part-2-process-synchronization">
<h2>Data distributed, part 2: process synchronization<a class="headerlink" href="#data-distributed-part-2-process-synchronization" title="Permalink to this headline">¶</a></h2>
<p>Now that we understand the initialization process, we can start filling out the body of the train method that does all of the work.</p>
<p>Recall what we have so far:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="n">init_process</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2"> process initialized.</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="c1"># rest of the training script goes here!</span>
</pre></div>
</div>
<p>Each of our four training processes runs this function to completion, exiting out when it is finished. If we were to run this code right now (via <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">multi_init.py</span></code>), we would see something like the following printed out to our console:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ python multi_init.py
<span class="m">1</span>/4 process initialized.
<span class="m">3</span>/4 process initialized.
<span class="m">2</span>/4 process initialized.
<span class="m">4</span>/4 process initialized.
</pre></div>
</div>
<p>The processes are independently executed, and there are no guarantees about what state any one state is at any one point in the training loop. This requires making some careful changes to your initialization process.</p>
<p><strong>(1)</strong> Any methods that download data should be isolated to the master process.</p>
<p>Failing to do so will replicate the download process across all of the processes, resulting in four processes writing to the same file simultaneously, wasting a lot of time and possibly risking a filesystem deadlock. This can be done as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># import torch.distributed as dist</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">downloading_dataset</span><span class="p">()</span>
    <span class="n">downloading_model_weights</span><span class="p">()</span>
<span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Rank </span><span class="si">{</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2"> training process passed data download barrier.</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">dist.barrier</span></code> call in this code sample will block until the master process (<code class="docutils literal notranslate"><span class="pre">rank</span> <span class="pre">==</span> <span class="pre">0</span></code>) is done <code class="docutils literal notranslate"><span class="pre">downloading_dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">downloading_model_weights</span></code>. This isolates all of the network I/O to a single process and prevents the other processes from jumping ahead until it’s done.</p>
<p><strong>(2)</strong> The data loader needs to use <code class="docutils literal notranslate"><span class="pre">DistributedSampler</span></code>. Code sample:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_dataloader</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">PascalVOCSegmentationDataset</span><span class="p">()</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span>
        <span class="n">dataset</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span>
    <span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">DistributedSampler</span></code> uses <code class="docutils literal notranslate"><span class="pre">rank</span></code> and <code class="docutils literal notranslate"><span class="pre">world_size</span></code> to partition the dataset across the processes into non-overlapping batches. Every training step the worker process retrieves <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> observations from its local copy of the dataset. In the example case of four GPUs, this means an effective batch size of <code class="docutils literal notranslate"><span class="pre">8</span> <span class="pre">*</span> <span class="pre">4</span> <span class="pre">=</span> <span class="pre">32</span></code>.</p>
<p><strong>(3)</strong> Tensors needs to be loaded into the correct device. To do so, parameterize your <code class="docutils literal notranslate"><span class="pre">.cuda()</span></code> or <code class="docutils literal notranslate"><span class="pre">.to()</span></code> calls with the rank of the device the process is managing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
<span class="n">segmap</span> <span class="o">=</span> <span class="n">segmap</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>(4)</strong> Any randomness in model initialization must be disabled.</p>
<p>It’s extremely important that the models start and remain synchronized throughout the entire training process. Otherwise, you’ll get inaccurate gradients and the model will fail to converge.</p>
<p>Random initialization methods like <code class="docutils literal notranslate"><span class="pre">torch.nn.init.kaiming_normal_</span></code> can be made deterministic using the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>The PyTorch documentation has an entire <a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html">Reproducibility</a> page dedicated to this topic.</p>
<p><strong>(5)</strong> Any methods that perform file I/O should be isolated to the master process.</p>
<p>This is necessary for the same reason that isolating network I/O is necessary: the inefficiency and potential for data corruption created by concurrent writes to the same file. Again, this is easy to do using simple conditional logic:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;/spell/checkpoints/&#39;</span><span class="p">):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s1">&#39;/spell/checkpoints/&#39;</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="sa">f</span><span class="s1">&#39;/spell/checkpoints/model_</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">.pth&#39;</span>
        <span class="p">)</span>
</pre></div>
</div>
<p>As an aside, note that any global loss values or statistics you want to log will require you to synchronize the data yourself. This can be done using additional MPI primitives in <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> not covered in-depth in this tutorial. Check out <a class="reference external" href="https://gist.github.com/ResidentMario/dc542fc26a142a9dce85b258835c45ad">this gist I prepared</a> for a quick intro, and refer to the <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html">Distributed Communication Package</a> PyTorch docs page for a detailed API reference.</p>
<p><strong>(6)</strong> The model must be wrapped in <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code>.</p>
<p>Assuming you’ve done everything else correctly, this is where the magic happens. ✨</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>
</pre></div>
</div>
<p>With that change the model is now training in distributed data parallel mode!</p>
<p><a class="reference external" href="https://github.com/spellrun/deeplab-voc-2012">To see complete code samples, head to the GitHub repo</a>.</p>
</div>
<div class="section" id="what-about-dataparallel">
<h2>What about DataParallel?<a class="headerlink" href="#what-about-dataparallel" title="Permalink to this headline">¶</a></h2>
<p>Readers familiar with the PyTorch API may know that there is also one other data parallelization strategy in PyTorch, <code class="docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code>. This API is much easier to use; all you have to do is wrap your model initialization like so:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>A one-liner change! Why not just use that instead?</p>
<p>Under the hood, <code class="docutils literal notranslate"><span class="pre">DataParallel</span></code> uses multithreading, instead of multiprocessing, to manage its GPU worker. This greatly simplifies the implementation: since the workers are all different threads of the same process, they all have access to the same shared state without requiring any additional synchronization steps.</p>
<p>However, using multithreading for computational jobs in Python is famously unperformant, due to the presence of the <a class="reference external" href="https://wiki.python.org/moin/GlobalInterpreterLock">Global Interpreter Lock</a>. As the benchmarks in the next section will show, models parallelized using <code class="docutils literal notranslate"><span class="pre">DataParallel</span></code> are significantly slower than those parallelized using <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code>.</p>
</div>
<div class="section" id="benchmarks">
<h2>Benchmarks<a class="headerlink" href="#benchmarks" title="Permalink to this headline">¶</a></h2>
<p>To benchmark distributed model training performance I trained a <code class="docutils literal notranslate"><span class="pre">DeepLabV3-ResNet</span> <span class="pre">101</span></code> model (via <a class="reference external" href="https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/">Torch Hub</a>) on the PASCAL VOC 2012 dataset (from <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> <a class="reference external" href="https://pytorch.org/docs/stable/torchvision/datasets.html">datasets</a>) for 20 epochs. I used the Spell API to launch five different versions of this model training job: once on a single V100 (a <code class="docutils literal notranslate"><span class="pre">p3.2xlarge</span></code> on AWS), and once each on a V100x4 (<code class="docutils literal notranslate"><span class="pre">p3.8xlarge</span></code>) and a V100x8 (<code class="docutils literal notranslate"><span class="pre">p3.16xlarge</span></code>) using <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> and <code class="docutils literal notranslate"><span class="pre">DataParallel</span></code>. This benchmark excludes the time spent downloading data at the beginning of the run—only model training and saving time counts.</p>
<p>The results are not definitive by any means, but should nevertheless give you some sense of the time save distributed training nets you:</p>
<p><em>Author note: these benchmarks were last run in June 2020. Improvements in the DistributedDataParallel implementation have likely further reduced runtimes since then.</em></p>
<p><img alt="Benchmarks" src="_images/benchmarks.avif" /></p>
<p>As you can clearly see, <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> is noticeably more efficient than <code class="docutils literal notranslate"><span class="pre">DataParallel</span></code>, but still far from perfect. Switching from a V100x1 to a V100x4 is a 4x multiplier on raw GPU power but only 3x on model training speed. Doubling the compute further by moving up to a V100x8 only produces a ~30% improvement in training speed. By that point <code class="docutils literal notranslate"><span class="pre">DataParallel</span></code> almost catches up to <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> in (in)efficiency.</p>
<p>Note that this is still an active area of development. Expect these times to continue to improve in new PyTorch releases!</p>
<!--
## To-do

- Redo the benchmarks.
- Has anything changed in this part of the API?
-->
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="mixed-precision.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Mixed Precision</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="model-distributed-training.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Model-Distributed Training</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Aleksey Bilogur<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>